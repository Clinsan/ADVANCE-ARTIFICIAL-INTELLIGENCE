{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrrLGIaL+eVldlBu6zQ3Ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clinsan/ADVANCE-ARTIFICIAL-INTELLIGENCE/blob/main/AAI_LAB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ADVANCE AI LAB ASSIGNMENT 1"
      ],
      "metadata": {
        "id": "qAvA09oo3Zv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:Clinsan Fernandes**<br>\n",
        "**PRN NO:20190802008**<br>\n",
        "**Aim: To perform segmentation and Feature Engineering on Text data.**<br>\n",
        "Segment to Extract: Title and Journal Name.<br>\n",
        "Steps to perform:<br>\n",
        "Data Collection.<br>\n",
        "Segments Extraction GroupWise. You can consider synonyms if you don't find the relative keywords.<br>\n",
        "Data Preprocessing.<br>\n",
        "Feature Engineering."
      ],
      "metadata": {
        "id": "Ifws1XNA3mJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98OFKFBblQnx",
        "outputId": "bbffcfab-f1b8-48aa-9936-444e93d5dabc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.11.0-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.11.0\n"
          ]
        }
      ],
      "source": [
        "#installing required libraries\n",
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import PyPDF2 as p\n",
        "#reading the pdfs and extracting the first page as title is required\n",
        "pdf1 = (p.PdfFileReader(\"/content/1.pdf\")).getPage(0).extractText()\n",
        "pdf2 = (p.PdfFileReader(\"/content/2.pdf\")).getPage(0).extractText()\n",
        "pdf3 = (p.PdfFileReader(\"/content/3.pdf\")).getPage(0).extractText()"
      ],
      "metadata": {
        "id": "akxH-HB_laBg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Segment Extraction (Title and Journal Name)"
      ],
      "metadata": {
        "id": "Wc1GH7TZsy7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting strings to list\n",
        "pdf1 = pdf1.splitlines()\n",
        "pdf2 = pdf2.splitlines()\n",
        "pdf3 = pdf3.splitlines()"
      ],
      "metadata": {
        "id": "mCIsQJf-n5T6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf1)\n",
        "print(pdf2)\n",
        "print(pdf3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja1T2160rB5c",
        "outputId": "edb3008d-6bd1-4fc2-fb44-97ced4d1291a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0162-8828 (c) 2016 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See', 'http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2644615, IEEE', 'Transactions on Pattern Analysis and Machine Intelligence', '1', 'SegNet: A Deep Convolutional', 'Encoder-Decoder Architecture for Scene', 'Segmentation', 'Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, Senior Member, IEEE,', 'Abstract—We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation', 'termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed', 'by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the', 'VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature', 'maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input', 'feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to', 'perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then', 'convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2]', 'and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus', 'accuracy trade-off involved in achieving good segmentation performance.', 'SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and', 'computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing', 'architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet', 'and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments', 'show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared', 'to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.', 'Index Terms—Deep Convolutional Neural Networks, Semantic Pixel-Wise Segmentation, Indoor Scenes, Road Scenes, Encoder,', 'Decoder, Pooling, Upsampling.', 'F', '1 I NTRODUCTION', 'Semantic segmentation has a wide array of applications ranging', 'from scene understanding, inferring support-relationships among', 'objects to autonomous driving. Early methods that relied on low-', 'level vision cues have fast been superseded by popular machine', 'learning algorithms. In particular, deep learning has seen huge suc-', 'cess lately in handwritten digit recognition, speech, categorising', 'whole images and detecting objects in images [5], [6]. Now there', 'is an active interest for semantic pixel-wise labelling [7] [8], [9],', '[2], [4], [10], [11], [12], [13], [3], [14], [15], [16]. However, some', 'of these recent approaches have tried to directly adopt deep archi-', 'tectures designed for category prediction to pixel-wise labelling', '[7]. The results, although very encouraging, appear coarse [3].', 'This is primarily because max pooling and sub-sampling reduce', 'feature map resolution. Our motivation to design SegNet arises', 'from this need to map low resolution features to input resolution', 'for pixel-wise classiﬁcation. This mapping must produce features', 'which are useful for accurate boundary localization.', 'Our architecture, SegNet, is designed to be an efﬁcient ar-', 'chitecture for pixel-wise semantic segmentation. It is primarily', 'motivated by road scene understanding applications which require', 'the ability to model appearance (road, building), shape (cars,', '\\x0fV . Badrinarayanan, A. Kendall, R. Cipolla are with the Machine Intelli-', 'gence Lab, Department of Engineering, University of Cambridge, UK.', 'E-mail: vb292,agk34,cipolla@eng.cam.ac.ukpedestrians) and understand the spatial-relationship (context) be-', 'tween different classes such as road and side-walk. In typical road', 'scenes, the majority of the pixels belong to large classes such', 'as road, building and hence the network must produce smooth', 'segmentations. The engine must also have the ability to delineate', 'objects based on their shape despite their small size. Hence it is', 'important to retain boundary information in the extracted image', 'representation. From a computational perspective, it is necessary', 'for the network to be efﬁcient in terms of both memory and', 'computation time during inference. The ability to train end-to-end', 'in order to jointly optimise all the weights in the network using', 'an efﬁcient weight update technique such as stochastic gradient', 'descent (SGD) [17] is an additional beneﬁt since it is more easily', 'repeatable. The design of SegNet arose from a need to match these', 'criteria.', 'The encoder network in SegNet is topologically identical to', 'the convolutional layers in VGG16 [1]. We remove the fully', 'connected layers of VGG16 which makes the SegNet encoder', 'network signiﬁcantly smaller and easier to train than many other', 'recent architectures [2], [4], [11], [18]. The key component of', 'SegNet is the decoder network which consists of a hierarchy', 'of decoders one corresponding to each encoder. Of these, the', 'appropriate decoders use the max-pooling indices received from', 'the corresponding encoder to perform non-linear upsampling of', 'their input feature maps. This idea was inspired from an archi-', 'tecture designed for unsupervised feature learning [19]. Reusing', 'max-pooling indices in the decoding process has several practical']\n",
            "['0162-8828 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2021.3059968, IEEE', 'Transactions on Pattern Analysis and Machine Intelligence', 'TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. ??, NO.??, ?? 2021 1', 'Image Segmentation Using Deep Learning:', 'A Survey', 'Shervin Minaee, Member, IEEE, Yuri Boykov, Member, IEEE, Fatih Porikli, Fellow, IEEE,', 'Antonio Plaza, Fellow, IEEE, Nasser Kehtarnavaz, Fellow, IEEE, and Demetri Terzopoulos, Fellow, IEEE', 'Abstract—Image segmentation is a key task in computer vision and image processing with important applications such as scene', 'understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others,', 'and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has', 'prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this', 'recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling', 'networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and', 'generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation', 'models, examine the widely used datasets, compare performances, and discuss promising research directions.', 'Index Terms—Image segmentation, deep learning, convolutional neural networks, encoder-decoder models, recurrent models,', 'generative models, semantic segmentation, instance segmentation, panoptic segmentation, medical image segmentation.', 'F', '1 I NTRODUCTION', 'IMAGE segmentation has been a fundamental problem in', 'computer vision since the early days of the ﬁeld [1] (Chap-', 'ter 8). An essential component of many visual understanding', 'systems, it involves partitioning images (or video frames)', 'into multiple segments and objects [2] (Chapter 5) and plays', 'a central role in a broad range of applications [3] (Part VI),', 'including medical image analysis (e.g., tumor boundary', 'extraction and measurement of tissue volumes), autonomous', 'vehicles (e.g., navigable surface and pedestrian detection),', 'video surveillance, and augmented reality to name a few.', 'Image segmentation can be formulated as the problem', 'of classifying pixels with semantic labels (semantic seg-', 'mentation), or partitioning of individual objects (instance', 'segmentation), or both (panoptic segmentation). Semantic', 'segmentation performs pixel-level labeling with a set of', 'object categories (e.g., human, car, tree, sky) for all image', 'pixels; thus, it is generally a more demanding undertaking', 'than whole-image classiﬁcation, which predicts a single label', 'for the entire image. Instance segmentation extends the scope', 'of semantic segmentation by detecting and delineating each', 'object of interest in the image (e.g., individual people).', 'Numerous image segmentation algorithms have been', 'developed in the literature, from the earliest methods,', 'such as thresholding [4], histogram-based bundling, region-', 'growing [5], k-means clustering [6], watershed methods [7],', 'to more advanced algorithms such as active contours [8],', 'graph cuts [9], conditional and Markov random ﬁelds [10],', 'and sparsity-based [11], [12] methods. In recent years,', 'however, deep learning (DL) models have yielded a new', '\\x0fS. Minaee is with Snapchat Machine Learning Research.', '\\x0fY. Boykov is with the University of Waterloo.', '\\x0fF. Porikli is with the Australian National University, and Huawei.', '\\x0fA. Plaza is with the University of Extremadura, Spain.', '\\x0fN. Kehtarnavaz is with the University of Texas at Dallas.', '\\x0fD. Terzopoulos is with the University of California, Los Angeles.', 'Manuscript received December ??, 2019; revised ?? ??, 2021.', 'Fig. 1. Segmentation results of DeepLabV3 [13] on sample images.', 'generation of image segmentation models with remarkable', 'performance improvements, often achieving the highest', 'accuracy rates on popular benchmarks (e.g., Fig. 1). This', 'has caused a paradigm shift in the ﬁeld.', 'This survey, a revised version of [14], covers the recent', 'literature in deep-learning-based image segmentation, includ-', 'ing more than 100 such segmentation methods proposed to', 'date. It provides a comprehensive review with insights into', 'different aspects of these methods, including the training', 'data, the choice of network architectures, loss functions,', 'training strategies, and their key contributions. The target', 'literature is organized into the following categories:', '1) Fully convolutional networks', '2) Convolutional models with graphical models', '3) Encoder-decoder based models', '4) Multiscale and pyramid network based models', '5) R-CNN based models (for instance segmentation)', '6) Dilated convolutional models and DeepLab family', '7) Recurrent neural network based models', 'Authorized licensed use limited to: Carleton University. Downloaded on May 28,2021 at 14:10:11 UTC from IEEE Xplore.  Restrictions apply. ']\n",
            "['0162-8828 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more', 'information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI', '10.1109/TPAMI.2016.2577031, IEEE Transactions on Pattern Analysis and Machine Intelligence', '1', 'Faster R-CNN: Towards Real-Time Object', 'Detection with Region Proposal Networks', 'Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun', 'Abstract—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.', 'Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region', 'proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image', 'convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional', 'network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to', 'generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN', 'into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with', '’attention’ mechanisms, the RPN component tells the uniﬁed network where to look. For the very deep VGG-16 model [3],', 'our detection system has a frame rate of 5fps (including all steps ) on a GPU, while achieving state-of-the-art object detection', 'accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO', '2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been', 'made publicly available.', 'Index Terms—Object Detection, Region Proposal, Convolutional Neural Network.', 'F', '1 I NTRODUCTION 1', 'Recent advances in object detection are driven by 2', 'the success of region proposal methods (e.g., [4]) 3', 'and region-based convolutional neural networks (R- 4', 'CNNs) [5]. Although region-based CNNs were com- 5', 'putationally expensive as originally developed in [5], 6', 'their cost has been drastically reduced thanks to shar- 7', 'ing convolutions across proposals [1], [2]. The latest 8', 'incarnation, Fast R-CNN [2], achieves near real-time 9', 'rates using very deep networks [3], when ignoring the 10', 'time spent on region proposals. Now, proposals are the 11', 'test-time computational bottleneck in state-of-the-art 12', 'detection systems. 13', 'Region proposal methods typically rely on inex- 14', 'pensive features and economical inference schemes. 15', 'Selective Search [4], one of the most popular meth- 16', 'ods, greedily merges superpixels based on engineered 17', 'low-level features. Yet when compared to efﬁcient 18', 'detection networks [2], Selective Search is an order of 19', 'magnitude slower, at 2 seconds per image in a CPU 20', 'implementation. EdgeBoxes [6] currently provides the 21', 'best tradeoff between proposal quality and speed, 22', 'at 0.2 seconds per image. Nevertheless, the region 23', 'proposal step still consumes as much running time 24', 'as the detection network. 25', '\\x0fS. Ren is with University of Science and Technology of China, Hefei,', 'China. This work was done when S. Ren was an intern at Microsoft', 'Research. Email: sqren@mail.ustc.edu.cn', '\\x0fK. He and J. Sun are with Visual Computing Group, Microsoft', 'Research. E-mail:fkahe,jiansung@microsoft.com', '\\x0fR. Girshick is with Facebook AI Research. The majority of this work', 'was done when R. Girshick was with Microsoft Research. E-mail:', 'rbg@fb.comOne may note that fast region-based CNNs take 26', 'advantage of GPUs, while the region proposal meth- 27', 'ods used in research are implemented on the CPU, 28', 'making such runtime comparisons inequitable. An ob- 29', 'vious way to accelerate proposal computation is to re- 30', 'implement it for the GPU. This may be an effective en- 31', 'gineering solution, but re-implementation ignores the 32', 'down-stream detection network and therefore misses 33', 'important opportunities for sharing computation. 34', 'In this paper, we show that an algorithmic change— 35', 'computing proposals with a deep convolutional neu- 36', 'ral network—leads to an elegant and effective solution 37', 'where proposal computation is nearly cost-free given 38', 'the detection network’s computation. To this end, we 39', 'introduce novel Region Proposal Networks (RPNs) that 40', 'share convolutional layers with state-of-the-art object 41', 'detection networks [1], [2]. By sharing convolutions at 42', 'test-time, the marginal cost for computing proposals 43', 'is small (e.g., 10ms per image). 44', 'Our observation is that the convolutional feature 45', 'maps used by region-based detectors, like Fast R- 46', 'CNN, can also be used for generating region pro- 47', 'posals. On top of these convolutional features, we 48', 'construct an RPN by adding a few additional con- 49', 'volutional layers that simultaneously regress region 50', 'bounds and objectness scores at each location on a 51', 'regular grid. The RPN is thus a kind of fully convo- 52', 'lutional network (FCN) [7] and can be trained end-to- 53', 'end speciﬁcally for the task for generating detection 54', 'proposals. 55', 'RPNs are designed to efﬁciently predict region pro- 56', 'posals with a wide range of scales and aspect ratios. In 57', 'contrast to prevalent methods [8], [9], [1], [2] that use 58']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting title and merging them into a single string\n",
        "pdf1=\"\".join(pdf1[4:7])\n",
        "pdf2=\"\".join(pdf2[3:5])\n",
        "pdf3=\"\".join(pdf3[4:6])\n"
      ],
      "metadata": {
        "id": "1XVx7olgoRqo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf1)\n",
        "print(pdf2)\n",
        "print(pdf3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osOyaPQuoX1K",
        "outputId": "730bde32-331e-45e0-94ef-8b4e89a1d417"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SegNet: A Deep ConvolutionalEncoder-Decoder Architecture for SceneSegmentation\n",
            "Image Segmentation Using Deep Learning:A Survey\n",
            "Faster R-CNN: Towards Real-Time ObjectDetection with Region Proposal Networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing & Feature Engineering"
      ],
      "metadata": {
        "id": "gFfgUrPGtZIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting text to lowe case for feature extraction\n",
        "pdf1 = pdf1.lower()\n",
        "pdf2 = pdf2.lower()\n",
        "pdf3 = pdf3.lower()\n",
        "print(pdf1)\n",
        "print(pdf2)\n",
        "print(pdf3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-43wSqxLoYkw",
        "outputId": "36b1e272-26c4-42fc-d570-54194f67b232"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segnet: a deep convolutionalencoder-decoder architecture for scenesegmentation\n",
            "image segmentation using deep learning:a survey\n",
            "faster r-cnn: towards real-time objectdetection with region proposal networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop words and punctuations"
      ],
      "metadata": {
        "id": "kU2TRzNruL_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def process_data(txt):\n",
        "  doc = nlp(txt)\n",
        "  filtered_words = []\n",
        "  for word in doc:\n",
        "    if word.is_stop or word.is_punct:\n",
        "      continue\n",
        "    filtered_words.append(word.lemma_)\n",
        "  return \" \".join(filtered_words)"
      ],
      "metadata": {
        "id": "sbbxpuX4ohCD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 =process_data(pdf1)\n",
        "data2 =process_data(pdf2)\n",
        "data3 =process_data(pdf3)"
      ],
      "metadata": {
        "id": "xiyCpMhqoiCF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding space and printing data\n",
        "data1=data1[:25] + \" \" + data1[25:]\n",
        "data1=data1[:60] + \" \" + data1[60:]\n",
        "data3=data3[:27] + \" \" + data3[27:]\n",
        "print(data1)\n",
        "print(data2)\n",
        "print(data3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haXpU9cjoj9H",
        "outputId": "aa314853-2b63-4f9d-e986-6101ebdaae17"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "segnet deep convolutional  encoder decoder architecture scen e segmentation\n",
            "image segmentation deep learning survey\n",
            "fast r cnn real time object  detection region proposal network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction"
      ],
      "metadata": {
        "id": "GevHWuh54cUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting string to vector fromat for the model\n",
        "data1_list = data1.split()\n",
        "data2_list = data2.split()\n",
        "data3_list = data3.split()\n",
        "print(data1_list,data2_list,data3_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBhdoAkk4045",
        "outputId": "ae0ade35-a1f2-4b7a-f812-ea53b527e9b4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['segnet', 'deep', 'convolutional', 'encoder', 'decoder', 'architecture', 'scen', 'e', 'segmentation'] ['image', 'segmentation', 'deep', 'learning', 'survey'] ['fast', 'r', 'cnn', 'real', 'time', 'object', 'detection', 'region', 'proposal', 'network']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#creating transformer\n",
        "vectorizer = CountVectorizer()\n",
        "#Train the Model by fitting the text_documents.\n",
        "vectorizer.fit([data1,data2,data3])\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJxabzTW4fwa",
        "outputId": "cc78bbec-b332-4fdb-9322-044e6a309971"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'segnet': 17, 'deep': 4, 'convolutional': 2, 'encoder': 6, 'decoder': 3, 'architecture': 0, 'scen': 15, 'segmentation': 16, 'image': 8, 'learning': 9, 'survey': 18, 'fast': 7, 'cnn': 1, 'real': 13, 'time': 19, 'object': 11, 'detection': 5, 'region': 14, 'proposal': 12, 'network': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing documents in the model\n",
        "vector1=vectorizer.transform([data1])\n",
        "vector2=vectorizer.transform([data2])\n",
        "vector3=vectorizer.transform([data3])\n",
        "#encoded vector\n",
        "print(vector1.toarray())\n",
        "print(vector2.toarray())\n",
        "print(vector3.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hhiddPS6FtL",
        "outputId": "4cb4b5bb-1f95-4e20-9684-ba5e1f500db8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0]]\n",
            "[[0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0]]\n",
            "[[0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observation\n",
        "In the above array we can see that the words present have the value 1 and others have 0. therefore common words between documents then the value will become 1 else if not common then value will become 0."
      ],
      "metadata": {
        "id": "jRs_pXVb5boF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction using TF-IDF method"
      ],
      "metadata": {
        "id": "WDJiDtTqxhES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#creating transformer\n",
        "vectorizer = TfidfVectorizer()\n",
        "#Train the Model by fitting the text_documents.\n",
        "vectorizer.fit([data1,data2,data3])\n",
        "print(vectorizer.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mBAamUAww5z",
        "outputId": "8f6448d7-f72c-4095-84df-4dc5d089b1a3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'segnet': 17, 'deep': 4, 'convolutional': 2, 'encoder': 6, 'decoder': 3, 'architecture': 0, 'scen': 15, 'segmentation': 16, 'image': 8, 'learning': 9, 'survey': 18, 'fast': 7, 'cnn': 1, 'real': 13, 'time': 19, 'object': 11, 'detection': 5, 'region': 14, 'proposal': 12, 'network': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observation:  \n",
        "words that appear frequently have the lowest values and unique words have the higest value."
      ],
      "metadata": {
        "id": "-czP_fH1zTS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing documents in the model\n",
        "vector1=vectorizer.transform([data1])\n",
        "vector2=vectorizer.transform([data2])\n",
        "vector3=vectorizer.transform([data3])\n",
        "#encoded vector\n",
        "print(vector1.toarray())\n",
        "print(vector2.toarray())\n",
        "print(vector3.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFwlxw9Dz5ha",
        "outputId": "f9e1b661-c2e3-4d89-a24c-5102397759e0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.37380112 0.         0.37380112 0.37380112 0.28428538 0.\n",
            "  0.37380112 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.37380112 0.28428538 0.37380112\n",
            "  0.         0.        ]]\n",
            "[[0.         0.         0.         0.         0.37302199 0.\n",
            "  0.         0.         0.49047908 0.49047908 0.         0.\n",
            "  0.         0.         0.         0.         0.37302199 0.\n",
            "  0.49047908 0.        ]]\n",
            "[[0.         0.33333333 0.         0.         0.         0.33333333\n",
            "  0.         0.33333333 0.         0.         0.33333333 0.33333333\n",
            "  0.33333333 0.33333333 0.33333333 0.         0.         0.\n",
            "  0.         0.33333333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observation\n",
        "The TF-IDF method gives us the importance of words present in the document.This method is preferred over the CountVectorizer() method."
      ],
      "metadata": {
        "id": "m7RJB7WO2UgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "successfully extracted the title and journal name from the given PDF's and performed data preprocessing and feature engineering"
      ],
      "metadata": {
        "id": "Z2fDKvlE118O"
      }
    }
  ]
}